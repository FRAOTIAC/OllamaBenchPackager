# 车载语音助手 Ollama 使用说明

## 📋 前置要求

确保已安装 [Ollama](https://ollama.com/)：
```bash
# 检查ollama是否安装
ollama --version
```

## ⚠️ 重要说明

### 模型命名规范
- Ollama 模型名称**不支持中文字符**，必须使用英文
- 只能包含小写字母、数字、连字符 `-` 和下划线 `_`
- 推荐命名：`vehicle-assistant`、`car-voice-helper` 等

### 跨平台兼容性
- ✅ **完全支持跨平台**：在 x86 主机创建的模型可以直接在 ARM 设备上运行
- ✅ **自动架构适配**：Ollama 会自动下载适合目标架构的模型权重
- ✅ **配置文件通用**：Modelfile 在不同架构间完全兼容

## 🚀 快速开始

### 1. 创建模型

将 `车载语音助手.Modelfile` 保存到本地，然后执行：

```bash
# 创建车载语音助手模型（注意：模型名称必须使用英文）
ollama create vehicle-assistant -f 车载语音助手.Modelfile
```

### 2. 运行模型

```bash
# 启动车载语音助手
ollama run vehicle-assistant
```

### 3. 测试使用

模型启动后，可以直接输入车辆控制指令：

```
>>> 把空调打开
{"u_message":"嗖——空调已经启动啦！凉风正往你这边吹呢，感觉如何？","veh_object":"air_switch","veh_operation":"open"}

>>> 空调开大点
{"u_message":"好的好的，风量已经调大啦！现在更凉爽了～","veh_object":"air_volume","veh_operation":"up"}

>>> 温度调高点
{"u_message":"好的好的，空调温度已经调高，现在应该更舒服了，是不是暖洋洋的？","veh_object":"air_temp","veh_operation":"up"}
```

## 🎯 支持的功能

### 空调控制
- "把空调打开" → 打开空调
- "关闭空调" → 关闭空调
- "空调开大点" → 调大风量
- "空调开小点" → 调小风量
- "温度调高点" → 调高温度
- "温度调低点" → 调低温度

### 车窗控制
- "打开车窗" → 打开车窗
- "关闭车窗" → 关闭车窗
- "车窗升起来" → 车窗上升
- "车窗降下去" → 车窗下降

### 灯光控制
- "打开远光灯" → 打开远光灯
- "关闭远光灯" → 关闭远光灯
- "开启近光灯" → 打开近光灯
- "关闭近光灯" → 关闭近光灯

## 📝 输出格式

所有响应都采用标准JSON格式：

```json
{
  "u_message": "给用户的友好回复",
  "veh_object": "车辆功能标识",
  "veh_operation": "操作指令"
}
```

### 功能标识 (veh_object)
- `air_switch`: 空调开关
- `air_volume`: 空调风量
- `air_temp`: 空调温度
- `car_window`: 车窗控制
- `dms_control`: 驾驶员状态检测
- `poslgt_control`: 位置灯
- `hbeam_control`: 远光灯
- `lbeam_control`: 近光灯

### 操作指令 (veh_operation)
- `open`: 打开/启动
- `close`: 关闭/停止
- `up`: 调大/调高/升起
- `down`: 调小/调低/降下

## 🔧 模型参数说明

针对 `qwen2.5:0.5b` 模型的优化参数：

- `temperature: 0.2` - 降低创造性，提高一致性
- `top_p: 0.7` - 减少随机性
- `top_k: 30` - 限制候选词数量
- `num_ctx: 2048` - 上下文窗口大小
- `num_predict: 128` - 最大预测tokens数

## 🛠️ 高级用法

### 通过API调用

```bash
# 使用ollama API
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vehicle-assistant",
    "prompt": "把空调打开",
    "stream": false
  }'
```

### 集成到应用

```python
import requests
import json

def call_voice_assistant(user_input):
    response = requests.post('http://localhost:11434/api/generate', 
                           json={
                               'model': 'vehicle-assistant',
                               'prompt': user_input,
                               'stream': False
                           })
    
    if response.status_code == 200:
        result = response.json()
        return json.loads(result['response'])
    return None

# 使用示例
result = call_voice_assistant("把空调打开")
print(result)
```

## 🚫 常见问题

### Q: 创建模型时提示 "Error: invalid model name" 怎么办？
A: 这是因为使用了中文字符作为模型名称。请使用英文名称：
```bash
# ❌ 错误
ollama create 车载助手 -f 车载语音助手.Modelfile

# ✅ 正确
ollama create vehicle-assistant -f 车载语音助手.Modelfile
```

### Q: 提示 "gathering model components" 后出错怎么办？
A: 通常是基础模型不存在，请先拉取基础模型：
```bash
ollama pull qwen2.5:0.5b
```

### Q: 模型输出格式不正确怎么办？
A: 确保使用了完整的 Modelfile，特别是 MESSAGE 示例对话部分。

### Q: 模型理解不准确怎么办？
A: 可以尝试调整 temperature 参数，或者增加更多 MESSAGE 示例。

### Q: 如何添加新的车辆功能？
A: 修改 SYSTEM 提示词中的功能选项，并添加相应的 MESSAGE 示例。

### Q: 在ARM设备上使用是否需要重新创建模型？
A: 不需要。将 Modelfile 复制到 ARM 设备后重新执行 create 命令即可，Ollama 会自动处理架构差异。

## 🔧 故障排除

### 快速诊断命令
```bash
# 1. 检查 Ollama 版本
ollama --version

# 2. 检查可用模型
ollama list

# 3. 拉取基础模型
ollama pull qwen2.5:0.5b

# 4. 创建自定义模型
ollama create vehicle-assistant -f 车载语音助手.Modelfile

# 5. 验证模型运行
ollama run vehicle-assistant

# 6. 测试API接口
curl -X POST http://localhost:11434/api/generate \
  -H "Content-Type: application/json" \
  -d '{"model": "vehicle-assistant", "prompt": "测试", "stream": false}'
```

### 性能优化建议
- **ARM设备**：建议调整 `num_thread` 参数适配CPU核心数
- **内存受限**：可以考虑使用更小的模型或调整 `num_ctx` 参数
- **响应速度**：降低 `temperature` 值可以提高响应一致性

## 📚 相关资源

- [Ollama官方文档](https://ollama.readthedocs.io/)
- [Modelfile参考](https://ollama.readthedocs.io/en/modelfile/)
- [Qwen2.5模型信息](https://ollama.com/library/qwen2.5)

## 🤝 贡献

欢迎提交改进建议和问题报告！

## 🌐 后台服务配置

### OpenAI API 兼容性
Ollama 完全支持 OpenAI 兼容的 API 接口，可以无缝替代 OpenAI API：

```bash
# OpenAI 兼容端点
http://localhost:11434/v1/chat/completions

# 原生 API 端点
http://localhost:11434/api/generate
```

### 启动后台服务
```bash
# 启动服务
ollama serve

# 后台运行
nohup ollama serve > ollama.log 2>&1 &
```

### 网络配置
```bash
# 配置网络访问（允许前端调用）
export OLLAMA_HOST="0.0.0.0"
export OLLAMA_ORIGINS="*"

# 重启服务
sudo systemctl restart ollama
```

### 前端集成示例
```javascript
// JavaScript 调用示例
const response = await fetch('http://localhost:11434/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    model: 'vehicle-assistant',
    messages: [{ role: 'user', content: '帮我打开空调' }],
    stream: false
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

**详细的后台服务配置请参考：`Ollama后台服务配置指南.md`**

---

*基于 qwen2:0.5b 模型构建，针对车载语音助手场景优化*  
*模型名称：vehicle-assistant（遵循 Ollama 英文命名规范）*  
*支持跨平台部署：x86、ARM 架构完全兼容*  
*支持 OpenAI API 兼容接口，便于前端集成* 